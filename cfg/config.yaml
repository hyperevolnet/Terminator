device: cuda
debug: False
deterministic: False
no_workers: 8
seed: 0

# network
net:
  type: "Terminator"
  no_hidden: 3   # 3 for rgb image
  final_no_hidden: 288
  no_blocks: 5
  dropout_type: Dropout2d
  dropout: [0.0, 0.05, 0.05, 0.05, 0.05]
  block:
    type: SFNE
    bottleneck_factors: [4, 3, 2, 2, 2]
    in_channel: [3, 12, 36, 72, 144]
    num_branch: 10   # 3 global + 4 local + 3 others
    num_concat_pre: [0, 0, 1, 1, 2]

# global branch
kernel_g:
  type: "MAGNet_G"
  num_layers: -1
  num_layers_list: [3, 2, 2, 2, 2]   # less layers in net's deeper layer
  no_hidden: 64
  size: 32   # image's height & width

# local branch
kernel_l:
  type: "MAGNet_L"
  no_hidden: 32
  num_layers: 2
  size: -1
  sizes: [3, 9, 5, 7]   # four local branch

# hyper-channel
kernel_gc:
  type: "MAGNet_GC"
  num_layers: 2
  no_hidden: 32
  size: -1   # dependent on number of channel

# hyper interact - channel
kernel_gc_hi:
  type: "MAGNet_GC"
  num_layers: 2
  no_hidden: 16
  size: -1   # dependent on number of channel

# hyper interact - spatial
kernel_gs_hi:
  type: "MAGNet_G"
  num_layers: 2
  no_hidden: 64
  size: 32   # image's height/width

# datamodules
dataset:
  name: 'CIFAR100'
  data_dir: '/data'
  data_type: 'image'

# training
train:
  do: True
  mixed_precision: False
  epochs: 100
  batch_size: 64
  grad_clip: 0
  max_epochs_no_improvement: 200
  track_grad_norm: -1 # -1 for no tracking
  accumulate_grad_steps: 1 # accumulate gradient over different batches
  distributed: False
  num_nodes: -1
  avail_gpus: -4
optimizer:
  name: Kar3
  betas: [0.75, 0.99] # [0.5, 0.95]
  lr: 0.00120925 # 0.001215
  l2_reg: 0.00004 # 0.00004
scheduler:
  name: 'cosine'
  warmup_epochs: 8
  mode: 'max'

# testing
test:
  batch_size_multiplier: 1
  before_train: False
# wandb logging
wandb:
  project: main
  entity: hyperzzw
# checkpoint
pretrained:
  load: False
  alias: 'best'
  filename: ""
